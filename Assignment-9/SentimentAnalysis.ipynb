{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd03d11d-15c5-4d63-bb6c-fc921a898f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a25b7dc-6b7d-4219-8e1a-392343b79b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Sentiment.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','sentiment']]\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e9c8c2e-12eb-496f-a872-b16099415af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aures\\AppData\\Local\\Temp\\ipykernel_11616\\1128855616.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  row[0] = row[0].replace('rt',' ')\n",
      "C:\\Users\\aures\\AppData\\Local\\Temp\\ipykernel_11616\\1128855616.py:2: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  row[0] = row[0].replace('rt',' ')\n"
     ]
    }
   ],
   "source": [
    "for idx, row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d539913f-de6a-4c53-9ddf-3914ba9b10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ') #Maximum words is 2000 to tokenize sentence\n",
    "tokenizer.fit_on_texts(data['text'].values) \n",
    "X = tokenizer.texts_to_sequences(data['text'].values) #taking values to feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e63114c-66ad-498c-b066-812ad1cb6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X) #Padding the feature matrix\n",
    "\n",
    "embed_dim = 128 #Dimension of the Embedded layer\n",
    "lstm_out = 196 #Long short-term memory (LSTM) layer neurons\n",
    "\n",
    "def createmodel():\n",
    "    model = Sequential() #Sequential Neural Network\n",
    "    model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1])) #input dimension 2000 Neurons, output dimension 128 Neurons\n",
    "    # Adding an additional LSTM layer with regularization\n",
    "    # model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, kernel_regularizer=l2(0.001)))\n",
    "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) #Drop out 20%, 196 output Neurons, recurrent dropout 20%\n",
    "    model.add(Dense(3,activation='softmax')) #3 output neurons[positive, Neutral, Negative], softmax as activation\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy']) #Compiling the model\n",
    "    return model\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "681580bc-d842-4e3e-b018-199d19d5ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder() #Applying label Encoding on the label matrix\n",
    "integer_encoded = labelencoder.fit_transform(data['sentiment']) #fitting the model\n",
    "y = to_categorical(integer_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42) #67% training data, 33% test data split\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4546f66-800f-463b-a217-43d4d1a80a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "291/291 - 15s - loss: 0.8193 - accuracy: 0.6465 - 15s/epoch - 53ms/step\n",
      "Epoch 2/2\n",
      "291/291 - 32s - loss: 0.6790 - accuracy: 0.7126 - 32s/epoch - 111ms/step\n",
      "144/144 - 3s - loss: 0.7427 - accuracy: 0.6828 - 3s/epoch - 23ms/step\n",
      "0.7426655292510986\n",
      "0.6828309297561646\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 #Batch size 32\n",
    "model = createmodel() #Function call to Sequential Neural Network\n",
    "model.fit(X_train, Y_train, epochs = 2, batch_size=batch_size, verbose = 2) #verbose the higher, the more messages\n",
    "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size) #evaluating the model\n",
    "print(score)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48210741-2da4-47f3-bf8e-a63ba7722258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names) #metrics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50dbbf65-d713-4413-a91e-16bcfba0a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('sentimentAnalysis.keras') #Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a18306fd-edf2-497f-a25f-39a82030f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model #Importing the package for importing the saved model\n",
    "model= load_model('sentimentAnalysis.keras') #loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48792cd6-4652-4afc-b4d7-0b3d9445a378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 ... 2 0 2]\n",
      "0         Neutral\n",
      "1        Positive\n",
      "2         Neutral\n",
      "3        Positive\n",
      "4        Positive\n",
      "           ...   \n",
      "13866    Negative\n",
      "13867    Positive\n",
      "13868    Positive\n",
      "13869    Negative\n",
      "13870    Positive\n",
      "Name: sentiment, Length: 13871, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(integer_encoded)\n",
    "print(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "184b4731-7855-485f-a432-c6718acc39f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s - 1s/epoch - 1s/step\n",
      "[0.63978845 0.14338629 0.21682528]\n",
      "0\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predicting on the text data\n",
    "sentence = ['A lot of good things are happening. We are respected again throughout the world, and that is a great thing.@realDonaldTrump']\n",
    "sentence = tokenizer.texts_to_sequences(sentence) # Tokenizing the sentence\n",
    "sentence = pad_sequences(sentence, maxlen=28, dtype='int32', value=0) # Padding the sentence\n",
    "sentiment_probs = model.predict(sentence, batch_size=1, verbose=2)[0] # Predicting the sentence text\n",
    "sentiment = np.argmax(sentiment_probs)\n",
    "\n",
    "print(sentiment_probs)\n",
    "print(sentiment)\n",
    "if sentiment == 0:\n",
    "    print(\"Neutral\")\n",
    "elif sentiment < 0:\n",
    "    print(\"Negative\")\n",
    "elif sentiment > 0:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Cannot be determined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638b19e-4dde-4c0f-800e-57f37a766c99",
   "metadata": {},
   "source": [
    "2. Apply GridSearchCV on the source code provided in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d4c3fe-14ea-457a-ae59-aabf7a723aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744/744 - 33s - loss: 0.8260 - accuracy: 0.6499 - 33s/epoch - 44ms/step\n",
      "186/186 - 2s - 2s/epoch - 10ms/step\n",
      "744/744 - 27s - loss: 0.8198 - accuracy: 0.6468 - 27s/epoch - 36ms/step\n",
      "186/186 - 2s - 2s/epoch - 9ms/step\n",
      "744/744 - 28s - loss: 0.8269 - accuracy: 0.6447 - 28s/epoch - 38ms/step\n",
      "186/186 - 2s - 2s/epoch - 11ms/step\n",
      "744/744 - 36s - loss: 0.8297 - accuracy: 0.6409 - 36s/epoch - 49ms/step\n",
      "186/186 - 3s - 3s/epoch - 14ms/step\n",
      "744/744 - 33s - loss: 0.8223 - accuracy: 0.6496 - 33s/epoch - 45ms/step\n",
      "186/186 - 2s - 2s/epoch - 12ms/step\n",
      "Epoch 1/2\n",
      "744/744 - 33s - loss: 0.8322 - accuracy: 0.6434 - 33s/epoch - 44ms/step\n",
      "Epoch 2/2\n",
      "744/744 - 28s - loss: 0.6805 - accuracy: 0.7092 - 28s/epoch - 38ms/step\n",
      "186/186 - 2s - 2s/epoch - 11ms/step\n",
      "Epoch 1/2\n",
      "744/744 - 37s - loss: 0.8194 - accuracy: 0.6462 - 37s/epoch - 50ms/step\n",
      "Epoch 2/2\n",
      "744/744 - 29s - loss: 0.6844 - accuracy: 0.7078 - 29s/epoch - 39ms/step\n",
      "186/186 - 2s - 2s/epoch - 13ms/step\n",
      "Epoch 1/2\n",
      "744/744 - 33s - loss: 0.8245 - accuracy: 0.6480 - 33s/epoch - 44ms/step\n",
      "Epoch 2/2\n",
      "744/744 - 28s - loss: 0.6781 - accuracy: 0.7104 - 28s/epoch - 38ms/step\n",
      "186/186 - 2s - 2s/epoch - 13ms/step\n",
      "Epoch 1/2\n",
      "744/744 - 28s - loss: 0.8276 - accuracy: 0.6480 - 28s/epoch - 37ms/step\n",
      "Epoch 2/2\n",
      "744/744 - 27s - loss: 0.6737 - accuracy: 0.7132 - 27s/epoch - 36ms/step\n",
      "186/186 - 2s - 2s/epoch - 10ms/step\n",
      "Epoch 1/2\n",
      "744/744 - 60s - loss: 0.8217 - accuracy: 0.6487 - 60s/epoch - 81ms/step\n",
      "Epoch 2/2\n",
      "744/744 - 59s - loss: 0.6697 - accuracy: 0.7116 - 59s/epoch - 80ms/step\n",
      "186/186 - 5s - 5s/epoch - 27ms/step\n",
      "372/372 - 38s - loss: 0.8360 - accuracy: 0.6404 - 38s/epoch - 102ms/step\n",
      "93/93 - 3s - 3s/epoch - 33ms/step\n",
      "372/372 - 44s - loss: 0.8268 - accuracy: 0.6431 - 44s/epoch - 118ms/step\n",
      "93/93 - 4s - 4s/epoch - 48ms/step\n",
      "372/372 - 37s - loss: 0.8336 - accuracy: 0.6398 - 37s/epoch - 99ms/step\n",
      "93/93 - 3s - 3s/epoch - 34ms/step\n",
      "372/372 - 36s - loss: 0.8352 - accuracy: 0.6416 - 36s/epoch - 96ms/step\n",
      "93/93 - 2s - 2s/epoch - 27ms/step\n",
      "372/372 - 28s - loss: 0.8215 - accuracy: 0.6463 - 28s/epoch - 75ms/step\n",
      "93/93 - 2s - 2s/epoch - 22ms/step\n",
      "Epoch 1/2\n",
      "372/372 - 39s - loss: 0.8305 - accuracy: 0.6419 - 39s/epoch - 104ms/step\n",
      "Epoch 2/2\n",
      "372/372 - 38s - loss: 0.6861 - accuracy: 0.7094 - 38s/epoch - 102ms/step\n",
      "93/93 - 4s - 4s/epoch - 38ms/step\n",
      "Epoch 1/2\n",
      "372/372 - 35s - loss: 0.8282 - accuracy: 0.6442 - 35s/epoch - 93ms/step\n",
      "Epoch 2/2\n",
      "372/372 - 33s - loss: 0.6855 - accuracy: 0.7090 - 33s/epoch - 89ms/step\n",
      "93/93 - 3s - 3s/epoch - 30ms/step\n",
      "Epoch 1/2\n",
      "372/372 - 35s - loss: 0.8269 - accuracy: 0.6418 - 35s/epoch - 93ms/step\n",
      "Epoch 2/2\n",
      "372/372 - 33s - loss: 0.6793 - accuracy: 0.7140 - 33s/epoch - 88ms/step\n",
      "93/93 - 3s - 3s/epoch - 32ms/step\n",
      "Epoch 1/2\n",
      "372/372 - 45s - loss: 0.8356 - accuracy: 0.6385 - 45s/epoch - 121ms/step\n",
      "Epoch 2/2\n",
      "372/372 - 68s - loss: 0.6782 - accuracy: 0.7143 - 68s/epoch - 183ms/step\n",
      "93/93 - 6s - 6s/epoch - 62ms/step\n",
      "Epoch 1/2\n",
      "372/372 - 38s - loss: 0.8310 - accuracy: 0.6408 - 38s/epoch - 103ms/step\n",
      "Epoch 2/2\n",
      "372/372 - 46s - loss: 0.6750 - accuracy: 0.7106 - 46s/epoch - 124ms/step\n",
      "93/93 - 4s - 4s/epoch - 47ms/step\n",
      "186/186 - 29s - loss: 0.8414 - accuracy: 0.6361 - 29s/epoch - 155ms/step\n",
      "47/47 - 3s - 3s/epoch - 62ms/step\n",
      "186/186 - 28s - loss: 0.8466 - accuracy: 0.6364 - 28s/epoch - 151ms/step\n",
      "47/47 - 3s - 3s/epoch - 60ms/step\n",
      "186/186 - 34s - loss: 0.8509 - accuracy: 0.6328 - 34s/epoch - 185ms/step\n",
      "47/47 - 4s - 4s/epoch - 75ms/step\n",
      "186/186 - 27s - loss: 0.8490 - accuracy: 0.6285 - 27s/epoch - 144ms/step\n",
      "47/47 - 3s - 3s/epoch - 59ms/step\n",
      "186/186 - 30s - loss: 0.8407 - accuracy: 0.6418 - 30s/epoch - 162ms/step\n",
      "47/47 - 3s - 3s/epoch - 63ms/step\n",
      "Epoch 1/2\n",
      "186/186 - 28s - loss: 0.8539 - accuracy: 0.6348 - 28s/epoch - 150ms/step\n",
      "Epoch 2/2\n",
      "186/186 - 25s - loss: 0.6999 - accuracy: 0.7007 - 25s/epoch - 132ms/step\n",
      "47/47 - 3s - 3s/epoch - 62ms/step\n",
      "Epoch 1/2\n",
      "186/186 - 35s - loss: 0.8334 - accuracy: 0.6412 - 35s/epoch - 190ms/step\n",
      "Epoch 2/2\n",
      "186/186 - 33s - loss: 0.6897 - accuracy: 0.7034 - 33s/epoch - 178ms/step\n",
      "47/47 - 2s - 2s/epoch - 52ms/step\n",
      "Epoch 1/2\n",
      "186/186 - 31s - loss: 0.8395 - accuracy: 0.6355 - 31s/epoch - 166ms/step\n",
      "Epoch 2/2\n",
      "186/186 - 28s - loss: 0.6750 - accuracy: 0.7146 - 28s/epoch - 151ms/step\n",
      "47/47 - 3s - 3s/epoch - 64ms/step\n",
      "Epoch 1/2\n",
      "186/186 - 29s - loss: 0.8429 - accuracy: 0.6344 - 29s/epoch - 157ms/step\n",
      "Epoch 2/2\n",
      "186/186 - 26s - loss: 0.6905 - accuracy: 0.7085 - 26s/epoch - 142ms/step\n",
      "47/47 - 3s - 3s/epoch - 61ms/step\n",
      "Epoch 1/2\n",
      "186/186 - 25s - loss: 0.8448 - accuracy: 0.6319 - 25s/epoch - 134ms/step\n",
      "Epoch 2/2\n",
      "186/186 - 23s - loss: 0.6810 - accuracy: 0.7080 - 23s/epoch - 123ms/step\n",
      "47/47 - 3s - 3s/epoch - 54ms/step\n",
      "Epoch 1/2\n",
      "233/233 - 37s - loss: 0.8265 - accuracy: 0.6410 - 37s/epoch - 157ms/step\n",
      "Epoch 2/2\n",
      "233/233 - 33s - loss: 0.6773 - accuracy: 0.7097 - 33s/epoch - 141ms/step\n",
      "Best: 0.680189 using {'batch_size': 40, 'epochs': 2}\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikeras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier #importing Keras classifier\n",
    "from sklearn.model_selection import GridSearchCV #importing Grid search CV\n",
    "\n",
    "model = KerasClassifier(model=createmodel,verbose=2) #initiating model to test performance by applying multiple hyper parameters\n",
    "batch_size= [10, 20, 40] #hyper parameter batch_size\n",
    "epochs = [1, 2] #hyper parameter no. of epochs\n",
    "param_grid= {'batch_size':batch_size, 'epochs':epochs} #creating dictionary for batch size, no. of epochs\n",
    "grid  = GridSearchCV(estimator=model, param_grid=param_grid) #Applying dictionary with hyper parameters\n",
    "grid_result= grid.fit(X_train,Y_train) #Fitting the model\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) #best score, best hyper parameters\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6530a-cefc-4d70-a0f1-d88339c220e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
